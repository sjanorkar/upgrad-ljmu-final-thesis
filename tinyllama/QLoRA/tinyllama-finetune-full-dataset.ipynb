{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-04T11:04:11.222157Z",
     "iopub.status.busy": "2024-08-04T11:04:11.221813Z",
     "iopub.status.idle": "2024-08-04T11:06:08.542129Z",
     "shell.execute_reply": "2024-08-04T11:06:08.541048Z",
     "shell.execute_reply.started": "2024-08-04T11:04:11.222128Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes==0.41.3\n",
      "  Downloading bitsandbytes-0.41.3-py3-none-any.whl.metadata (9.8 kB)\n",
      "Downloading bitsandbytes-0.41.3-py3-none-any.whl (92.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.41.3\n",
      "Collecting peft==0.12.0\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from peft==0.12.0) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from peft==0.12.0) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from peft==0.12.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from peft==0.12.0) (6.0.1)\n",
      "Collecting torch>=1.13.0 (from peft==0.12.0)\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting transformers (from peft==0.12.0)\n",
      "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from peft==0.12.0) (4.66.4)\n",
      "Collecting accelerate>=0.21.0 (from peft==0.12.0)\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting safetensors (from peft==0.12.0)\n",
      "  Downloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting huggingface-hub>=0.17.0 (from peft==0.12.0)\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2024.6.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.12.0) (4.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->peft==0.12.0) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.12.0) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.12.0)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers->peft==0.12.0) (2024.5.15)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers->peft==0.12.0)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.12.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.12.0) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.12.0) (1.3.0)\n",
      "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.5/435.5 kB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, accelerate, peft\n",
      "Successfully installed accelerate-0.33.0 huggingface-hub-0.24.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 peft-0.12.0 safetensors-0.4.4 tokenizers-0.19.1 torch-2.4.0 transformers-4.44.0 triton-3.0.0\n",
      "Collecting trl==0.8.6\n",
      "  Downloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trl==0.8.6) (2.4.0)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trl==0.8.6) (4.44.0)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trl==0.8.6) (1.22.4)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from trl==0.8.6) (0.33.0)\n",
      "Collecting datasets (from trl==0.8.6)\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tyro>=0.5.11 (from trl==0.8.6)\n",
      "  Downloading tyro-0.8.8-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (4.12.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (2024.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.8.6) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl==0.8.6) (12.6.20)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.6) (0.24.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.6) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.6) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.6) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.6) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.6) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.6) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.8.6) (4.66.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl==0.8.6)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.6)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate->trl==0.8.6) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->trl==0.8.6) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->trl==0.8.6) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->trl==0.8.6) (2.2.2)\n",
      "Collecting xxhash (from datasets->trl==0.8.6)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->trl==0.8.6) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets->trl==0.8.6) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.6) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.6) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.6) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.6) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.6) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.8.6) (4.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.8.6) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.6) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.6) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.6) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.8.6) (2024.6.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.8.6) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.6) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.6) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets->trl==0.8.6) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.8.6) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.8.6) (1.16.0)\n",
      "Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.8.8-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, shtab, docstring-parser, tyro, datasets, trl\n",
      "Successfully installed datasets-2.21.0 docstring-parser-0.16 shtab-1.7.1 trl-0.8.6 tyro-0.8.8 xxhash-3.4.1\n",
      "Collecting datasets==2.19.2\n",
      "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (1.22.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (0.70.16)\n",
      "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.2)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (0.24.5)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets==2.19.2) (6.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.19.2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.19.2) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.19.2) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.19.2) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.19.2) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->datasets==2.19.2) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets==2.19.2) (4.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging->datasets==2.19.2) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.32.1->datasets==2.19.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.32.1->datasets==2.19.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.32.1->datasets==2.19.2) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.32.1->datasets==2.19.2) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets==2.19.2) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets==2.19.2) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets==2.19.2) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.2) (1.16.0)\n",
      "Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.0\n",
      "    Uninstalling fsspec-2024.6.0:\n",
      "      Successfully uninstalled fsspec-2024.6.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.21.0\n",
      "    Uninstalling datasets-2.21.0:\n",
      "      Successfully uninstalled datasets-2.21.0\n",
      "Successfully installed datasets-2.19.2 fsspec-2024.3.1\n",
      "Collecting transformers==4.43.3\n",
      "  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.43.3) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.43.3) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.43.3) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.43.3) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.43.3) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.43.3) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.43.3) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.43.3) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.43.3) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers==4.43.3) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.3) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.3) (4.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.43.3) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.43.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.43.3) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.43.3) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers==4.43.3) (2024.6.2)\n",
      "Downloading transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.0\n",
      "    Uninstalling transformers-4.44.0:\n",
      "      Successfully uninstalled transformers-4.44.0\n",
      "Successfully installed transformers-4.43.3\n",
      "Collecting tensorboard==2.17.0\n",
      "  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard==2.17.0)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard==2.17.0)\n",
      "  Downloading grpcio-1.65.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard==2.17.0)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorboard==2.17.0) (1.22.4)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorboard==2.17.0) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorboard==2.17.0) (70.0.0)\n",
      "Requirement already satisfied: six>1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorboard==2.17.0) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard==2.17.0)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tensorboard==2.17.0) (3.0.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard==2.17.0) (2.1.5)\n",
      "Downloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.65.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m143.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Installing collected packages: tensorboard-data-server, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.1.0 grpcio-1.65.4 markdown-3.6 tensorboard-2.17.0 tensorboard-data-server-0.7.2\n",
      "Requirement already satisfied: accelerate==0.33.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.33.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate==0.33.0) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate==0.33.0) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate==0.33.0) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate==0.33.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate==0.33.0) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate==0.33.0) (0.24.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate==0.33.0) (0.4.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2024.3.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.33.0) (4.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.33.0) (3.1.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.33.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.33.0) (12.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.33.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.33.0) (2024.6.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.33.0) (1.3.0)\n",
      "Collecting sentencepiece==0.2.0\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bitsandbytes==0.41.3\n",
    "!pip3 install peft==0.12.0\n",
    "!pip3 install trl==0.8.6\n",
    "!pip3 install datasets==2.19.2\n",
    "!pip3 install transformers==4.43.3\n",
    "!pip3 install tensorboard==2.17.0\n",
    "!pip3 install accelerate==0.33.0\n",
    "!pip3 install sentencepiece==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:15:22.367980Z",
     "iopub.status.busy": "2024-08-04T11:15:22.367587Z",
     "iopub.status.idle": "2024-08-04T11:15:39.867432Z",
     "shell.execute_reply": "2024-08-04T11:15:39.866674Z",
     "shell.execute_reply.started": "2024-08-04T11:15:22.367944Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:15:47.258431Z",
     "iopub.status.busy": "2024-08-04T11:15:47.257646Z",
     "iopub.status.idle": "2024-08-04T11:15:47.262606Z",
     "shell.execute_reply": "2024-08-04T11:15:47.261393Z",
     "shell.execute_reply.started": "2024-08-04T11:15:47.258395Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"Vezora/Tested-22k-Python-Alpaca\"\n",
    "model_id = \"TinyLlama/TinyLlama_v1.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:15:49.265171Z",
     "iopub.status.busy": "2024-08-04T11:15:49.264529Z",
     "iopub.status.idle": "2024-08-04T11:15:49.270248Z",
     "shell.execute_reply": "2024-08-04T11:15:49.269259Z",
     "shell.execute_reply.started": "2024-08-04T11:15:49.265140Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id, \n",
    "        trust_remote_code=True, \n",
    "        use_fast=False,\n",
    "        add_eos_token=True,\n",
    "        add_bos_token=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization config for QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:15:51.637392Z",
     "iopub.status.busy": "2024-08-04T11:15:51.636708Z",
     "iopub.status.idle": "2024-08-04T11:15:51.643117Z",
     "shell.execute_reply": "2024-08-04T11:15:51.642188Z",
     "shell.execute_reply.started": "2024-08-04T11:15:51.637361Z"
    }
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:15:53.241725Z",
     "iopub.status.busy": "2024-08-04T11:15:53.241369Z",
     "iopub.status.idle": "2024-08-04T11:15:53.247655Z",
     "shell.execute_reply": "2024-08-04T11:15:53.246678Z",
     "shell.execute_reply.started": "2024-08-04T11:15:53.241695Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model_id, local=False):\n",
    "    \n",
    "    if local:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            quantization_config=bnb_config,\n",
    "            local_files_only=True,\n",
    "            device_map={\"\": 0})\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            quantization_config=bnb_config,\n",
    "            device_map={\"\": 0})\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Inference Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:15:55.145553Z",
     "iopub.status.busy": "2024-08-04T11:15:55.145225Z",
     "iopub.status.idle": "2024-08-04T11:15:55.152359Z",
     "shell.execute_reply": "2024-08-04T11:15:55.151417Z",
     "shell.execute_reply.started": "2024-08-04T11:15:55.145527Z"
    }
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"\"\"Can you design a program in Python that can predict the likelihood of a certain material becoming unstable under high pressure and temperature conditions? This can be calculated using the Gurney equation, which takes into account parameters such as the chemical composition and crystal structure of the material. Additionally, can you provide a real-world scenario where this program could be applied in the field of material science and engineering?\"\"\",\n",
    "    \"\"\"Write a Python function that returns the maximum value of the given set of integers: 1, 5, 23, and 9. The function should only use one line of code and must utilize the lambda function. Additionally, the function should be able to handle any set of integers provided as input, and should return an error message if any non-integer values are detected. The output should be displayed in a formatted table with the following columns: \"Input Set\", \"Maximum Value\". The table should also include a row for the given set of integers as well as a row for a randomly generated set of 10 integers between 1 and 100. Lastly, the function should be written in SQL and should be able to query a database table containing the input sets and their corresponding maximum values.\"\"\",\n",
    "    \"\"\"Create a function that takes two parameters, a number and a string, and returns an array with those two values in it. The string parameter should be checked if it is a valid string and if it contains only alphabetical characters. If the string is not valid (length less than 10 characters) or contains non-alphabetical characters, the function should return an empty array. Additionally, the number parameter should be restricted to a range between -1000 and 1000 (inclusive). The function should also check if the number is a prime number and return an empty array if it is not.\"\"\",\n",
    "    \"\"\"Given a string, convert it to upper case using only basic string manipulation operations. The string may contain punctuation marks, special characters, and spaces. String: This string should be in upper case!\"\"\",\n",
    "    \"\"\"Find the sum of the first 1000 prime numbers that are greater than 100.\"\"\",\n",
    "    \"\"\"Provide a function that finds the length of a string, but without using any built-in string length functions or methods, and without using any iteration or recursion.\"\"\",\n",
    "    \"\"\"Create a function that removes duplicates from an array and returns an array of the unique values. The function should only use O(1) additional space and have a time complexity of O(n), where n is the length of the input array. The input array can contain integers, floating-point numbers, strings, and nested arrays. The output array should be sorted in descending order. Additionally, the function should handle nested arrays correctly by recursively flattening them before removing duplicates.\"\"\",\n",
    "    \"\"\"Create a 3x4 NumPy array of random integers from 0 to 5, where each row should have at least one unique value.\"\"\",\n",
    "    \"\"\"Find the index of the element 'c' in the following list, but the list may contain duplicates and the element 'c' may appear multiple times.\"\"\",\n",
    "    \"\"\"Write a function for finding the minimum value in a given array, with a time complexity requirement of O(n log n), where n is the length of the array.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:15:57.789880Z",
     "iopub.status.busy": "2024-08-04T11:15:57.789093Z",
     "iopub.status.idle": "2024-08-04T11:15:57.795388Z",
     "shell.execute_reply": "2024-08-04T11:15:57.794368Z",
     "shell.execute_reply.started": "2024-08-04T11:15:57.789849Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_inference(model, tokenizer):\n",
    "    pipe = pipeline(task='text-generation', model=model, tokenizer=tokenizer, max_length=256)\n",
    "\n",
    "    for prompt in prompts:\n",
    "        result = pipe(prompt)\n",
    "        print(\"******************************* PROMPT *******************************\")\n",
    "        print(prompt)\n",
    "        print(\"******************************* Inference *******************************\")\n",
    "        print(result[0]['generated_text'])\n",
    "        print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Fine-tuning inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:16:00.721267Z",
     "iopub.status.busy": "2024-08-04T11:16:00.720590Z",
     "iopub.status.idle": "2024-08-04T11:17:59.593686Z",
     "shell.execute_reply": "2024-08-04T11:17:59.592789Z",
     "shell.execute_reply.started": "2024-08-04T11:16:00.721235Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8c5d7affd1494ba1f852d46217e236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0657d9d4774a9fab4d24f5024855b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7051dd56d8a4f01a3cc489c2d01897b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0c0711fd164b57873630f3b78a5e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5b5a0a0afd4d5eb06d45668ec56357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604b72d8be4d4becb15dfc6a1d4d6d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831b69caef934675bc48662acda83e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************* PROMPT *******************************\n",
      "Can you design a program in Python that can predict the likelihood of a certain material becoming unstable under high pressure and temperature conditions? This can be calculated using the Gurney equation, which takes into account parameters such as the chemical composition and crystal structure of the material. Additionally, can you provide a real-world scenario where this program could be applied in the field of material science and engineering?\n",
      "******************************* Inference *******************************\n",
      "Can you design a program in Python that can predict the likelihood of a certain material becoming unstable under high pressure and temperature conditions? This can be calculated using the Gurney equation, which takes into account parameters such as the chemical composition and crystal structure of the material. Additionally, can you provide a real-world scenario where this program could be applied in the field of material science and engineering? The 2018-19 season is the 100th anniversary of the 1918-1919 season, and the 100th anniversary of the 1918-1919 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Write a Python function that returns the maximum value of the given set of integers: 1, 5, 23, and 9. The function should only use one line of code and must utilize the lambda function. Additionally, the function should be able to handle any set of integers provided as input, and should return an error message if any non-integer values are detected. The output should be displayed in a formatted table with the following columns: \"Input Set\", \"Maximum Value\". The table should also include a row for the given set of integers as well as a row for a randomly generated set of 10 integers between 1 and 100. Lastly, the function should be written in SQL and should be able to query a database table containing the input sets and their corresponding maximum values.\n",
      "******************************* Inference *******************************\n",
      "Write a Python function that returns the maximum value of the given set of integers: 1, 5, 23, and 9. The function should only use one line of code and must utilize the lambda function. Additionally, the function should be able to handle any set of integers provided as input, and should return an error message if any non-integer values are detected. The output should be displayed in a formatted table with the following columns: \"Input Set\", \"Maximum Value\". The table should also include a row for the given set of integers as well as a row for a randomly generated set of 10 integers between 1 and 100. Lastly, the function should be written in SQL and should be able to query a database table containing the input sets and their corresponding maximum values. The 2018-19 season is the 100th anniversary of the National Hockey League. To celebrate this milestone, the NHL is releasing a series of 100th Anniversary Collector's Cards.\n",
      "Each card features a different player from the NHL's 100-year history, and each card will be available for purchase\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Create a function that takes two parameters, a number and a string, and returns an array with those two values in it. The string parameter should be checked if it is a valid string and if it contains only alphabetical characters. If the string is not valid (length less than 10 characters) or contains non-alphabetical characters, the function should return an empty array. Additionally, the number parameter should be restricted to a range between -1000 and 1000 (inclusive). The function should also check if the number is a prime number and return an empty array if it is not.\n",
      "******************************* Inference *******************************\n",
      "Create a function that takes two parameters, a number and a string, and returns an array with those two values in it. The string parameter should be checked if it is a valid string and if it contains only alphabetical characters. If the string is not valid (length less than 10 characters) or contains non-alphabetical characters, the function should return an empty array. Additionally, the number parameter should be restricted to a range between -1000 and 1000 (inclusive). The function should also check if the number is a prime number and return an empty array if it is not. The 2018-19 season is the 100th anniversary of the 1918-1919 season, which was the first season of the NHL.\n",
      "The 1918-1919 season was the first season of the NHL.\n",
      "The 1918-1919 season was the first season of the NHL. The 1918-1919 season was the first season of the NHL.\n",
      "The 1918-1919 season was the first season of\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Given a string, convert it to upper case using only basic string manipulation operations. The string may contain punctuation marks, special characters, and spaces. String: This string should be in upper case!\n",
      "******************************* Inference *******************************\n",
      "Given a string, convert it to upper case using only basic string manipulation operations. The string may contain punctuation marks, special characters, and spaces. String: This string should be in upper case! The 2018-19 season is the 100th anniversary of the 1918-1919 season, and the 100th anniversary of the 1918-1919 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "The 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "The 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "The 2018-19 season is the 100th anniversary of the 1918-1\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Find the sum of the first 1000 prime numbers that are greater than 100.\n",
      "******************************* Inference *******************************\n",
      "Find the sum of the first 1000 prime numbers that are greater than 100. The 2018-19 season is the 100th anniversary of the founding of the National Hockey League. To celebrate this milestone, the NHL is releasing a series of 100-year-old photographs from the league's archives.\n",
      "Each week, the NHL will release a new photograph from its archives. The first week of the series features the 1918-1919 season, which was the first season of the NHL.\n",
      "The NHL's 100th anniversary season is the 100th anniversary of the founding of the National Hockey League. To celebrate this milestone, the NHL is releasing a series of 100-year-old photographs from the league's archives.\n",
      "Each week, the NHL will release a new photograph from its archives. The first week of the series features the 1918-1919 season, which was the first season of the NHL.\n",
      "The NHL's\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Provide a function that finds the length of a string, but without using any built-in string length functions or methods, and without using any iteration or recursion.\n",
      "******************************* Inference *******************************\n",
      "Provide a function that finds the length of a string, but without using any built-in string length functions or methods, and without using any iteration or recursion. The 2018-19 season is the 100th anniversary of the National Hockey League. To celebrate this milestone, the NHL is releasing a series of 100th Anniversary Collector's Cards.\n",
      "Each card features a different player from the NHL's 100-year history, and each card is numbered to 100.\n",
      "The cards will be available for purchase in the NHL Shop and at select NHL team stores.\n",
      "The cards will be available for purchase in the NHL Shop and at select NHL team stores.\n",
      "The cards will be available for purchase in the NHL Shop and at select NHL team stores.\n",
      "The cards will be available for purchase in the NHL Shop and at select NHL team stores.\n",
      "The cards will be available for purchase in the NHL Shop and at select NHL team stores.\n",
      "The cards will be available for purchase in the NHL Shop and at select NHL team stores\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************* PROMPT *******************************\n",
      "Create a function that removes duplicates from an array and returns an array of the unique values. The function should only use O(1) additional space and have a time complexity of O(n), where n is the length of the input array. The input array can contain integers, floating-point numbers, strings, and nested arrays. The output array should be sorted in descending order. Additionally, the function should handle nested arrays correctly by recursively flattening them before removing duplicates.\n",
      "******************************* Inference *******************************\n",
      "Create a function that removes duplicates from an array and returns an array of the unique values. The function should only use O(1) additional space and have a time complexity of O(n), where n is the length of the input array. The input array can contain integers, floating-point numbers, strings, and nested arrays. The output array should be sorted in descending order. Additionally, the function should handle nested arrays correctly by recursively flattening them before removing duplicates. The 2018-19 season is the 100th anniversary of the National Hockey League. To celebrate this milestone, the NHL is hosting a series of events and activities throughout the year.\n",
      "The NHL 100th Anniversary Celebration is a series of events and activities throughout the year that will celebrate the 100th anniversary of the National Hockey League.\n",
      "The NHL 100th Anniversary Celebration is a series of events and activities throughout the year that will celebrate the 100th anniversary of the National Hockey League. The NHL 100th Anniversary Celebration is a series of events and\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Create a 3x4 NumPy array of random integers from 0 to 5, where each row should have at least one unique value.\n",
      "******************************* Inference *******************************\n",
      "Create a 3x4 NumPy array of random integers from 0 to 5, where each row should have at least one unique value. The 2018-19 season is the 100th anniversary of the 1918-1919 season, and the 100th anniversary of the 1918-1919 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "The 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "The 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "The 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "The 2018\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Find the index of the element 'c' in the following list, but the list may contain duplicates and the element 'c' may appear multiple times.\n",
      "******************************* Inference *******************************\n",
      "Find the index of the element 'c' in the following list, but the list may contain duplicates and the element 'c' may appear multiple times. The 2018-19 season is the 100th anniversary of the 1918-1919 season, and the 100th anniversary of the 1918-1919 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-191\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Write a function for finding the minimum value in a given array, with a time complexity requirement of O(n log n), where n is the length of the array.\n",
      "******************************* Inference *******************************\n",
      "Write a function for finding the minimum value in a given array, with a time complexity requirement of O(n log n), where n is the length of the array. The 2018-19 season is the 100th anniversary of the 1918-1919 season, and the 100th anniversary of the 1918-1919 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-1919 season.\n",
      "Therefore, the 2018-19 season is the 100th anniversary of the 1918-\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = load_tokenizer(model_id)\n",
    "model = load_model(model_id)\n",
    "generate_inference(model, TOKENIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:19:13.512884Z",
     "iopub.status.busy": "2024-08-04T11:19:13.512488Z",
     "iopub.status.idle": "2024-08-04T11:19:14.667308Z",
     "shell.execute_reply": "2024-08-04T11:19:14.666552Z",
     "shell.execute_reply.started": "2024-08-04T11:19:13.512854Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb5221b0c894732bc0996b639927cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea32fca21b9841cab58746238a71f22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/49.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8b224429254bce8be2af734bd38528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/22608 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "# instruction = dataset[\"train\"][\"instruction\"]\n",
    "# input = dataset[\"train\"][\"input\"]\n",
    "# output = dataset[\"train\"][\"output\"]\n",
    "\n",
    "# temp_dataset_1 = Dataset.from_dict({\"instruction\": instruction[:5500], \"input\": input[:5500], \"output\": output[:5500]})\n",
    "# dataset_1 = datasets.DatasetDict({\"train\": temp_dataset_1})\n",
    "\n",
    "# temp_dataset_2 = Dataset.from_dict({\"instruction\": instruction[5500:11000], \"input\": input[5500:11000], \"output\": output[5500:11000]})\n",
    "# dataset_2 = datasets.DatasetDict({\"train\": temp_dataset_2})\n",
    "\n",
    "# temp_dataset_3 = Dataset.from_dict({\"instruction\": instruction[11000:16500], \"input\": input[11000:16500], \"output\": output[11000:16500]})\n",
    "# dataset_3 = datasets.DatasetDict({\"train\": temp_dataset_3})\n",
    "\n",
    "# temp_dataset_4 = Dataset.from_dict({\"instruction\": instruction[16500:], \"input\": input[16500:], \"output\": output[16500:]})\n",
    "# dataset_4 = datasets.DatasetDict({\"train\": temp_dataset_4})\n",
    "\n",
    "\n",
    "# datasets = [dataset_1, dataset_2, dataset_3, dataset_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:19:19.243889Z",
     "iopub.status.busy": "2024-08-04T11:19:19.243145Z",
     "iopub.status.idle": "2024-08-04T11:19:19.248308Z",
     "shell.execute_reply": "2024-08-04T11:19:19.247161Z",
     "shell.execute_reply.started": "2024-08-04T11:19:19.243858Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    \"\"\"\n",
    "    Formatting function returning a list of samples (kind of necessary for SFT API).\n",
    "    \"\"\"\n",
    "    text = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n{example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:19:26.565713Z",
     "iopub.status.busy": "2024-08-04T11:19:26.565369Z",
     "iopub.status.idle": "2024-08-04T11:19:26.571126Z",
     "shell.execute_reply": "2024-08-04T11:19:26.570124Z",
     "shell.execute_reply.started": "2024-08-04T11:19:26.565686Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(dataset):\n",
    "    train_val = dataset[\"train\"].train_test_split(test_size=3000, shuffle=True, seed=42)\n",
    "    train_data = train_val[\"train\"]\n",
    "    val_data = train_val[\"test\"]\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:19:30.414741Z",
     "iopub.status.busy": "2024-08-04T11:19:30.414359Z",
     "iopub.status.idle": "2024-08-04T11:19:30.419323Z",
     "shell.execute_reply": "2024-08-04T11:19:30.418387Z",
     "shell.execute_reply.started": "2024-08-04T11:19:30.414709Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return TOKENIZER(preprocess_function(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:19:32.736252Z",
     "iopub.status.busy": "2024-08-04T11:19:32.735460Z",
     "iopub.status.idle": "2024-08-04T11:21:48.953733Z",
     "shell.execute_reply": "2024-08-04T11:21:48.952795Z",
     "shell.execute_reply.started": "2024-08-04T11:19:32.736218Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_data, val_data = train_test_split(dataset)\n",
    "\n",
    "# tokenized_train_dataset = train_data.map(generate_and_tokenize_prompt)\n",
    "# tokenized_val_dataset = val_data.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:21:58.742590Z",
     "iopub.status.busy": "2024-08-04T11:21:58.741722Z",
     "iopub.status.idle": "2024-08-04T11:21:58.747821Z",
     "shell.execute_reply": "2024-08-04T11:21:58.746730Z",
     "shell.execute_reply.started": "2024-08-04T11:21:58.742555Z"
    }
   },
   "outputs": [],
   "source": [
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "    \"lm_head\",\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MICRO_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "TRAIN_STEPS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:22:01.220260Z",
     "iopub.status.busy": "2024-08-04T11:22:01.219892Z",
     "iopub.status.idle": "2024-08-04T11:22:01.224752Z",
     "shell.execute_reply": "2024-08-04T11:22:01.223923Z",
     "shell.execute_reply.started": "2024-08-04T11:22:01.220231Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:22:03.483314Z",
     "iopub.status.busy": "2024-08-04T11:22:03.482961Z",
     "iopub.status.idle": "2024-08-04T11:22:03.866507Z",
     "shell.execute_reply": "2024-08-04T11:22:03.865604Z",
     "shell.execute_reply.started": "2024-08-04T11:22:03.483287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,160,448 || all params: 1,113,208,832 || trainable%: 1.1822\n"
     ]
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_params)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:22:06.907593Z",
     "iopub.status.busy": "2024-08-04T11:22:06.906932Z",
     "iopub.status.idle": "2024-08-04T11:22:06.936855Z",
     "shell.execute_reply": "2024-08-04T11:22:06.936038Z",
     "shell.execute_reply.started": "2024-08-04T11:22:06.907559Z"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR=\"/home/ec2-user/SageMaker/tinyllama\"\n",
    "#OUTPUT_DIR=\"/kaggle/working/tinyllama\"\n",
    "CONTEXT_LENGTH = 1024\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=OUTPUT_DIR,\n",
    "  num_train_epochs=3,\n",
    "  per_device_train_batch_size=4,\n",
    "  per_device_eval_batch_size=4,\n",
    "  gradient_accumulation_steps=4,\n",
    "  optim=\"paged_adamw_32bit\",\n",
    "  save_steps=300,\n",
    "  logging_steps=300,\n",
    "  learning_rate=0.0001,\n",
    "  eval_strategy=\"steps\",\n",
    "  weight_decay=0.001,\n",
    "  fp16=True,\n",
    "  bf16=False,\n",
    "  max_grad_norm=0.3,\n",
    "  max_steps=-1,\n",
    "  warmup_ratio=0.03,\n",
    "  group_by_length=True,\n",
    "  lr_scheduler_type=\"constant\",\n",
    "  report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:22:11.517694Z",
     "iopub.status.busy": "2024-08-04T11:22:11.517327Z",
     "iopub.status.idle": "2024-08-04T11:22:11.523821Z",
     "shell.execute_reply": "2024-08-04T11:22:11.522842Z",
     "shell.execute_reply.started": "2024-08-04T11:22:11.517664Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_dataset, eval_dataset, model, tokenizer, output_path):\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        max_seq_length=CONTEXT_LENGTH,\n",
    "        tokenizer=TOKENIZER,\n",
    "        args=training_args,\n",
    "        formatting_func=preprocess_function,\n",
    "        packing=True,\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(TOKENIZER, mlm=False),\n",
    "    )\n",
    "    \n",
    "    print(\"Model output path: \", f\"{output_path}/model/fine_tuned\")\n",
    "    \n",
    "    history = trainer.train()\n",
    "    model.save_pretrained(f\"{output_path}/model/fine_tuned\")\n",
    "    #tokenizer.save_pretrained(f\"{output_path}/tokenizer/fine_tuned\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:22:14.295951Z",
     "iopub.status.busy": "2024-08-04T11:22:14.295017Z",
     "iopub.status.idle": "2024-08-04T11:22:14.300437Z",
     "shell.execute_reply": "2024-08-04T11:22:14.299451Z",
     "shell.execute_reply.started": "2024-08-04T11:22:14.295913Z"
    }
   },
   "outputs": [],
   "source": [
    "# context_length = 1024\n",
    "    \n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     #train_dataset=tokenized_train_dataset,\n",
    "#     #eval_dataset=tokenized_val_dataset,\n",
    "#     train_dataset=train_data,\n",
    "#     eval_dataset=val_data,\n",
    "#     max_seq_length=context_length,\n",
    "#     tokenizer=TOKENIZER,\n",
    "#     args=training_args,\n",
    "#     formatting_func=preprocess_function,\n",
    "#     packing=True,\n",
    "#     data_collator=transformers.DataCollatorForLanguageModeling(TOKENIZER, mlm=False),\n",
    "# )\n",
    "\n",
    "# print(\"Model output path: \", f\"{OUTPUT_DIR}/model/fine_tuned\")\n",
    "\n",
    "# history = trainer.train()\n",
    "# model.save_pretrained(f\"{OUTPUT_DIR}/model/fine_tuned\")\n",
    "# #TOKENIZER.save_pretrained(f\"{OUTPUT_DIR}/fine_tuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune model with dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:22:16.976224Z",
     "iopub.status.busy": "2024-08-04T11:22:16.975572Z",
     "iopub.status.idle": "2024-08-04T11:22:16.981257Z",
     "shell.execute_reply": "2024-08-04T11:22:16.980414Z",
     "shell.execute_reply.started": "2024-08-04T11:22:16.976192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1723787898488\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "current_timestamp_ms = int(time.time() * 1000)\n",
    "print(current_timestamp_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:22:21.225453Z",
     "iopub.status.busy": "2024-08-04T11:22:21.225081Z",
     "iopub.status.idle": "2024-08-04T11:22:32.586044Z",
     "shell.execute_reply": "2024-08-04T11:22:32.584089Z",
     "shell.execute_reply.started": "2024-08-04T11:22:21.225423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,160,448 || all params: 1,113,208,832 || trainable%: 1.1822\n",
      "Fine-Tuning:  TinyLlama/TinyLlama_v1.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f024fcc9e09d481a85c64f1eaafbc710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd12846a4ae4da1b1a49c82ac10f4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output path:  /home/ec2-user/SageMaker/tinyllama/dataset-full/model/fine_tuned\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2163' max='2163' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2163/2163 2:09:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.250500</td>\n",
       "      <td>4.482464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.503400</td>\n",
       "      <td>6.569386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>7.497700</td>\n",
       "      <td>8.559399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>10.354400</td>\n",
       "      <td>21.364464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>52.596500</td>\n",
       "      <td>66.085098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>73.217900</td>\n",
       "      <td>81.321770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>87.437700</td>\n",
       "      <td>94.864334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/peft/utils/save_and_load.py:202: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/peft/utils/save_and_load.py:202: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/peft/utils/save_and_load.py:202: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/peft/utils/save_and_load.py:202: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/peft/utils/save_and_load.py:202: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/peft/utils/save_and_load.py:202: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/peft/utils/save_and_load.py:202: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/peft/utils/save_and_load.py:202: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1723803320381"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR = \"/home/ec2-user/SageMaker/tinyllama/dataset-\"\n",
    "#BASE_DIR = \"/kaggle/working/tinyllama/dataset-\"\n",
    "train_data, val_data = train_test_split(dataset)\n",
    "\n",
    "index = \"full\"\n",
    "model_id = \"TinyLlama/TinyLlama_v1.1\"\n",
    "model = load_model(model_id)\n",
    "model = get_peft_model(model, peft_params)\n",
    "model.print_trainable_parameters()\n",
    "print(\"Fine-Tuning: \", model_id)\n",
    "model = train(train_data, val_data, model, TOKENIZER, f\"{BASE_DIR}{index}\")\n",
    "\n",
    "current_timestamp_ms = int(time.time() * 1000)\n",
    "current_timestamp_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference after finetuning with dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************* PROMPT *******************************\n",
      "Can you design a program in Python that can predict the likelihood of a certain material becoming unstable under high pressure and temperature conditions? This can be calculated using the Gurney equation, which takes into account parameters such as the chemical composition and crystal structure of the material. Additionally, can you provide a real-world scenario where this program could be applied in the field of material science and engineering?\n",
      "******************************* Inference *******************************\n",
      "Can you design a program in Python that can predict the likelihood of a certain material becoming unstable under high pressure and temperature conditions? This can be calculated using the Gurney equation, which takes into account parameters such as the chemical composition and crystal structure of the material. Additionally, can you provide a real-world scenario where this program could be applied in the field of material science and engineering? ### Instruction:\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Write a Python function that returns the maximum value of the given set of integers: 1, 5, 23, and 9. The function should only use one line of code and must utilize the lambda function. Additionally, the function should be able to handle any set of integers provided as input, and should return an error message if any non-integer values are detected. The output should be displayed in a formatted table with the following columns: \"Input Set\", \"Maximum Value\". The table should also include a row for the given set of integers as well as a row for a randomly generated set of 10 integers between 1 and 100. Lastly, the function should be written in SQL and should be able to query a database table containing the input sets and their corresponding maximum values.\n",
      "******************************* Inference *******************************\n",
      "Write a Python function that returns the maximum value of the given set of integers: 1, 5, 23, and 9. The function should only use one line of code and must utilize the lambda function. Additionally, the function should be able to handle any set of integers provided as input, and should return an error message if any non-integer values are detected. The output should be displayed in a formatted table with the following columns: \"Input Set\", \"Maximum Value\". The table should also include a row for the given set of integers as well as a row for a randomly generated set of 10 integers between 1 and 100. Lastly, the function should be written in SQL and should be able to query a database table containing the input sets and their corresponding maximum values.``````\n",
      "\n",
      "# Input\n",
      "#                                                                                                                                                                                                                                          \n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Create a function that takes two parameters, a number and a string, and returns an array with those two values in it. The string parameter should be checked if it is a valid string and if it contains only alphabetical characters. If the string is not valid (length less than 10 characters) or contains non-alphabetical characters, the function should return an empty array. Additionally, the number parameter should be restricted to a range between -1000 and 1000 (inclusive). The function should also check if the number is a prime number and return an empty array if it is not.\n",
      "******************************* Inference *******************************\n",
      "Create a function that takes two parameters, a number and a string, and returns an array with those two values in it. The string parameter should be checked if it is a valid string and if it contains only alphabetical characters. If the string is not valid (length less than 10 characters) or contains non-alphabetical characters, the function should return an empty array. Additionally, the number parameter should be restricted to a range between -1000 and 1000 (inclusive). The function should also check if the number is a prime number and return an empty array if it is not.``````\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "### Response:\n",
      "\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Given a string, convert it to upper case using only basic string manipulation operations. The string may contain punctuation marks, special characters, and spaces. String: This string should be in upper case!\n",
      "******************************* Inference *******************************\n",
      "Given a string, convert it to upper case using only basic string manipulation operations. The string may contain punctuation marks, special characters, and spaces. String: This string should be in upper case!``````\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "\n",
      "## Response:\n",
      "\n",
      "\n",
      "## Input:\n",
      "\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Find the sum of the first 1000 prime numbers that are greater than 100.\n",
      "******************************* Inference *******************************\n",
      "Find the sum of the first 1000 prime numbers that are greater than 100. ### Instruction:\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "###\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Provide a function that finds the length of a string, but without using any built-in string length functions or methods, and without using any iteration or recursion.\n",
      "******************************* Inference *******************************\n",
      "Provide a function that finds the length of a string, but without using any built-in string length functions or methods, and without using any iteration or recursion.```````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************* PROMPT *******************************\n",
      "Create a function that removes duplicates from an array and returns an array of the unique values. The function should only use O(1) additional space and have a time complexity of O(n), where n is the length of the input array. The input array can contain integers, floating-point numbers, strings, and nested arrays. The output array should be sorted in descending order. Additionally, the function should handle nested arrays correctly by recursively flattening them before removing duplicates.\n",
      "******************************* Inference *******************************\n",
      "Create a function that removes duplicates from an array and returns an array of the unique values. The function should only use O(1) additional space and have a time complexity of O(n), where n is the length of the input array. The input array can contain integers, floating-point numbers, strings, and nested arrays. The output array should be sorted in descending order. Additionally, the function should handle nested arrays correctly by recursively flattening them before removing duplicates.```python\n",
      "\n",
      "# Input\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "#\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Create a 3x4 NumPy array of random integers from 0 to 5, where each row should have at least one unique value.\n",
      "******************************* Inference *******************************\n",
      "Create a 3x4 NumPy array of random integers from 0 to 5, where each row should have at least one unique value.``````\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "### Response   \n",
      "\n",
      "### Input   \n",
      "\n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "### Response   \n",
      "\n",
      "###\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Find the index of the element 'c' in the following list, but the list may contain duplicates and the element 'c' may appear multiple times.\n",
      "******************************* Inference *******************************\n",
      "Find the index of the element 'c' in the following list, but the list may contain duplicates and the element 'c' may appear multiple times.`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "====================================================================================================\n",
      "******************************* PROMPT *******************************\n",
      "Write a function for finding the minimum value in a given array, with a time complexity requirement of O(n log n), where n is the length of the array.\n",
      "******************************* Inference *******************************\n",
      "Write a function for finding the minimum value in a given array, with a time complexity requirement of O(n log n), where n is the length of the array.````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_ft = load_model(f\"{BASE_DIR}{index}/model/fine_tuned\", True)\n",
    "generate_inference(model_ft,TOKENIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune model with dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_timestamp_ms = int(time.time() * 1000)\n",
    "print(\"finetuning with dataset 2\", current_timestamp_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/home/ec2-user/SageMaker/tinyllama/dataset-\"\n",
    "#BASE_DIR = \"/kaggle/working/tinyllama/dataset-\"\n",
    "train_data, val_data = train_test_split(dataset_2)\n",
    "\n",
    "index = 2\n",
    "model_id = f\"{BASE_DIR}{index-1}/model/fine_tuned\"\n",
    "model = load_model(model_id, True)\n",
    "model = get_peft_model(model, peft_params)\n",
    "model.print_trainable_parameters()\n",
    "print(\"Fine-Tuning: \", model_id)\n",
    "model = train(train_data, val_data, model, TOKENIZER, f\"{BASE_DIR}{index}\")\n",
    "\n",
    "current_timestamp_ms = int(time.time() * 1000)\n",
    "current_timestamp_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference after finetuning with dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-01T09:51:10.053485Z",
     "iopub.status.idle": "2024-08-01T09:51:10.053936Z",
     "shell.execute_reply": "2024-08-01T09:51:10.053721Z",
     "shell.execute_reply.started": "2024-08-01T09:51:10.053705Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_ft = load_model(f\"{BASE_DIR}{index}/model/fine_tuned\", True)\n",
    "generate_inference(model_ft,TOKENIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune model with dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_timestamp_ms = int(time.time() * 1000)\n",
    "print(\"finetuning with dataset 3\", current_timestamp_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/home/ec2-user/SageMaker/tinyllama/dataset-\"\n",
    "#BASE_DIR = \"/kaggle/working/tinyllama/dataset-\"\n",
    "train_data, val_data = train_test_split(dataset_3)\n",
    "\n",
    "index = 3\n",
    "model_id = f\"{BASE_DIR}{index-1}/model/fine_tuned\"\n",
    "model = load_model(model_id)\n",
    "model = get_peft_model(model, peft_params)\n",
    "model.print_trainable_parameters()\n",
    "print(\"Fine-Tuning: \", model_id)\n",
    "model = train(train_data, val_data, model, TOKENIZER, f\"{BASE_DIR}{index}\")\n",
    "\n",
    "current_timestamp_ms = int(time.time() * 1000)\n",
    "current_timestamp_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference after finetuning with dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = load_model(f\"{BASE_DIR}{index}/model/fine_tuned\", True)\n",
    "generate_inference(model_ft,TOKENIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetune model with dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_timestamp_ms = int(time.time() * 1000)\n",
    "print(\"finetuning with dataset 4\", current_timestamp_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/home/ec2-user/SageMaker/tinyllama/dataset-\"\n",
    "#BASE_DIR = \"/kaggle/working/tinyllama/dataset-\"\n",
    "train_data, val_data = train_test_split(dataset_4)\n",
    "\n",
    "index = 4\n",
    "model_id = f\"{BASE_DIR}{index-1}/model/fine_tuned\"\n",
    "model = load_model(model_id)\n",
    "model = get_peft_model(model, peft_params)\n",
    "model.print_trainable_parameters()\n",
    "print(\"Fine-Tuning: \", model_id)\n",
    "model = train(train_data, val_data, model, TOKENIZER, f\"{BASE_DIR}{index}\")\n",
    "\n",
    "current_timestamp_ms = int(time.time() * 1000)\n",
    "current_timestamp_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference after finetuning with dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = load_model(f\"{BASE_DIR}{index}/model/fine_tuned\", True)\n",
    "generate_inference(model_ft,TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
